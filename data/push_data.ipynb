{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12653151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335d4ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aaron\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace with your actual token\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e7b25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/datasets/aaronmat1905/ADA_Project_Data', endpoint='https://huggingface.co', repo_type='dataset', repo_id='aaronmat1905/ADA_Project_Data')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "# Your token\n",
    "api = HfApi(token=\"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\")\n",
    "\n",
    "# Create repo\n",
    "repo_id = \"aaronmat1905/ADA_Project_Data\"\n",
    "api.create_repo(repo_id=repo_id, repo_type=\"dataset\", private=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a90eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ Hugging Face Data Upload Script\n",
      "======================================================================\n",
      "\n",
      "üì° Connecting to Hugging Face...\n",
      "\n",
      "üì¶ Creating/checking repository: aaronmat1905/ADA_Project_Data\n",
      "   ‚úì Repository ready: https://huggingface.co/datasets/aaronmat1905/ADA_Project_Data\n",
      "\n",
      "üìÇ Preparing upload tasks...\n",
      "\n",
      "üì§ Starting upload of 11 items...\n",
      "\n",
      "[1/11] üìÅ Data Dictionary\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[2/11] üìÅ Commodity Prices (Extracted)\n",
      "   üìä Size: 1.21 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CMO-Historical-Data-Annual.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 627k/627k [00:01<00:00, 413kB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[3/11] üìÅ CMO Data Supplement\n",
      "   üìä Size: 2.14 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CMO-April-2025-Agriculture.xlsx:   0%|          | 0.00/414k [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "CMO-April-2025-Agriculture.xlsx:   2%|‚ñè         | 8.19k/414k [00:00<00:09, 40.6kB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "CMO-April-2025-Agriculture.xlsx:  30%|‚ñà‚ñà‚ñâ       | 123k/414k [00:00<00:00, 462kB/s]  \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "CMO-April-2025-Precious-Metals.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101k/101k [00:00<00:00, 104kB/s]  \n",
      "CMO-April-2025-Special-Focus.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 361k/361k [00:01<00:00, 308kB/s]\n",
      "CMO-April-2025-Executive-Summary.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 604k/604k [00:01<00:00, 461kB/s]\n",
      "CMO-April-2025-Energy.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 575k/575k [00:01<00:00, 376kB/s]\n",
      "CMO-April-2025-Agriculture.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 414k/414k [00:01<00:00, 219kB/s]\n",
      "\n",
      "\n",
      "Upload 5 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[4/11] üìÅ Macroeconomic Data\n",
      "   üìä Size: 7.47 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Index of Industrial Production.xlsx:   0%|          | 0.00/259k [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "Index of Industrial Production.xlsx:  35%|‚ñà‚ñà‚ñà‚ñç      | 90.1k/259k [00:00<00:00, 901kB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "Index of Industrial Production.xlsx:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 197k/259k [00:00<00:00, 927kB/s] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Index of Industrial Production.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 259k/259k [00:01<00:00, 236kB/s]\n",
      "\n",
      "\n",
      "Other Macroeconomic Indicators.xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 355k/355k [00:01<00:00, 332kB/s] \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "RBIB Table No. 18 _ Consumer Price Index (Base 2010=100).xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 431k/431k [00:01<00:00, 245kB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Wholesale Price Index - Monthly Data .xlsx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.45M/6.45M [00:03<00:00, 1.84MB/s]\n",
      "\n",
      "Upload 4 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:03<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[5/11] üìÅ Trade Data (Small)\n",
      "   üìä Size: 2.79 MB\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[6/11] üìÅ Input-Output Data\n",
      "   üìä Size: 0.01 MB\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[7/11] üìÅ Instruments Data\n",
      "   üìä Size: 0.00 MB\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[8/11] üìÅ Large Trade File: dataset_2025-10-22T05_39_53.398513973Z_DEFAULT_INT...\n",
      "   üìä Size: 2917.09 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset_2025-10-22T05_39_53.398513973Z_DEFAULT_INTEGRATION_IMF.STA_IMTS_1.0.0.csv:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 2.14G/3.06G [22:40<07:04, 2.17MB/s]   '(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/6f/ba/6fba24dc8e3d626e17b34cff248e8774dc3611320b10a3f76d0aaed5c54ef64d/63f96cfce892379c465ae2f9a7b114c81c6c5a6041b5ee9b4e486e8622082a08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T094804Z&X-Amz-Expires=86400&X-Amz-Signature=532151e2db9cc0a012b80183e6a06a4ca5074c268e47c5c8bf1fc4a217d27bee&X-Amz-SignedHeaders=host&partNumber=134&uploadId=rG11xaLEuSzN5PXujtzZHX1qoJz.r3A_wqoBwJ2Td4uly0gx0IpvF.Ueq_D3f5iO330GYmZ2.eTsh0KsAKttn5tJAbhY1SzmSz9UtlU1W92WY9enX_G30_8AzJGJd0BH&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2406)')))\"), '(Request ID: a988f366-eb04-4da0-90fb-f9331889d4e9)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/6f/ba/6fba24dc8e3d626e17b34cff248e8774dc3611320b10a3f76d0aaed5c54ef64d/63f96cfce892379c465ae2f9a7b114c81c6c5a6041b5ee9b4e486e8622082a08?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20251022%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251022T094804Z&X-Amz-Expires=86400&X-Amz-Signature=532151e2db9cc0a012b80183e6a06a4ca5074c268e47c5c8bf1fc4a217d27bee&X-Amz-SignedHeaders=host&partNumber=134&uploadId=rG11xaLEuSzN5PXujtzZHX1qoJz.r3A_wqoBwJ2Td4uly0gx0IpvF.Ueq_D3f5iO330GYmZ2.eTsh0KsAKttn5tJAbhY1SzmSz9UtlU1W92WY9enX_G30_8AzJGJd0BH&x-id=UploadPart\n",
      "Retrying in 1s [Retry 1/5].\n",
      "dataset_2025-10-22T05_39_53.398513973Z_DEFAULT_INTEGRATION_IMF.STA_IMTS_1.0.0.csv: 3.07GB [24:32, 2.08MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[9/11] üìÅ Large Trade File: dataset_2025-10-22T07_56_33.238649022Z_DEFAULT_INT...\n",
      "   üìä Size: 2917.09 MB\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[10/11] üìÅ Large Trade File: INT-Export-10-22-2025_11-32-56.csv...\n",
      "   üìä Size: 4.97 MB\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "[11/11] üìÅ Large Trade File: INT-Export-10-22-2025_13-02-11.csv...\n",
      "   üìä Size: 4.11 MB\n",
      "   ‚úì Successfully uploaded\n",
      "\n",
      "======================================================================\n",
      "üìä UPLOAD SUMMARY\n",
      "======================================================================\n",
      "‚úì Successful: 11\n",
      "‚úó Failed: 0\n",
      "üì¶ Repository: https://huggingface.co/datasets/aaronmat1905/ADA_Project_Data\n",
      "======================================================================\n",
      "\n",
      "üéâ All data uploaded successfully!\n",
      "\n",
      "üí° Next steps:\n",
      "   1. Delete local data to free up space\n",
      "   2. Use the data in your code with:\n",
      "\n",
      "   from huggingface_hub import hf_hub_download\n",
      "   file = hf_hub_download(\n",
      "       repo_id='aaronmat1905/ADA_Project_Data',\n",
      "       filename='raw/commodity_prices/extracted/cmoMonthly_monthly_prices.csv',\n",
      "       repo_type='dataset'\n",
      "   )\n",
      "   df = pd.read_csv(file)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hugging Face Data Upload Script with Retry Logic and Progress Tracking\n",
    "Uploads your global-trade-shocks-analysis data to Hugging Face in chunks\n",
    "\"\"\"\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# ============= CONFIGURATION =============\n",
    "HF_TOKEN = \"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\"  # Replace with your token\n",
    "HF_USERNAME = \"aaronmat1905\"   # Replace with your HF username\n",
    "REPO_NAME = \"ADA_Project_Data\"\n",
    "BASE_PATH = r\"C:\\Users\\Aaron\\Documents\\aaronswork\\global-trade-shocks-analysis\\data\"\n",
    "\n",
    "# =========================================\n",
    "\n",
    "def upload_with_retry(api, upload_func, max_retries=3, delay=5, **kwargs):\n",
    "    \"\"\"Retry upload function with exponential backoff\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            upload_func(**kwargs)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = delay * (2 ** attempt)\n",
    "                print(f\"   ‚ö† Attempt {attempt + 1} failed: {str(e)[:100]}\")\n",
    "                print(f\"   ‚è≥ Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"   ‚úó Failed after {max_retries} attempts: {str(e)[:100]}\")\n",
    "                return False\n",
    "    return False\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    \"\"\"Calculate folder size in MB\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if os.path.exists(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ Hugging Face Data Upload Script\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize API\n",
    "    print(\"\\nüì° Connecting to Hugging Face...\")\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "    repo_id = f\"{HF_USERNAME}/{REPO_NAME}\"\n",
    "    \n",
    "    # Create repository\n",
    "    print(f\"\\nüì¶ Creating/checking repository: {repo_id}\")\n",
    "    try:\n",
    "        api.create_repo(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            private=True,\n",
    "            exist_ok=True\n",
    "        )\n",
    "        print(f\"   ‚úì Repository ready: https://huggingface.co/datasets/{repo_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚úó Failed to create repository: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Define upload tasks\n",
    "    print(\"\\nüìÇ Preparing upload tasks...\")\n",
    "    \n",
    "    # Small files and folders first\n",
    "    small_uploads = [\n",
    "        {\n",
    "            \"type\": \"file\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"data-dictionary.md\"),\n",
    "            \"remote\": \"data-dictionary.md\",\n",
    "            \"name\": \"Data Dictionary\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"folder\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"raw\", \"commodity_prices\", \"extracted\"),\n",
    "            \"remote\": \"raw/commodity_prices/extracted\",\n",
    "            \"name\": \"Commodity Prices (Extracted)\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"folder\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"raw\", \"commodity_prices\", \"CMO-April-2025-Data-Supplement\"),\n",
    "            \"remote\": \"raw/commodity_prices/CMO-April-2025-Data-Supplement\",\n",
    "            \"name\": \"CMO Data Supplement\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"folder\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"raw\", \"macroeconomic\"),\n",
    "            \"remote\": \"raw/macroeconomic\",\n",
    "            \"name\": \"Macroeconomic Data\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"folder\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"raw\", \"trade\"),\n",
    "            \"remote\": \"raw/trade\",\n",
    "            \"name\": \"Trade Data (Small)\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"folder\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"raw\", \"input_output\"),\n",
    "            \"remote\": \"raw/input_output\",\n",
    "            \"name\": \"Input-Output Data\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"folder\",\n",
    "            \"local\": os.path.join(BASE_PATH, \"raw\", \"instruments\"),\n",
    "            \"remote\": \"raw/instruments\",\n",
    "            \"name\": \"Instruments Data\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Large IMF trade files separately\n",
    "    large_trade_folder = os.path.join(BASE_PATH, \"raw\", \"data\", \"raw\", \"trade\")\n",
    "    if os.path.exists(large_trade_folder):\n",
    "        for filename in os.listdir(large_trade_folder):\n",
    "            if filename.endswith('.csv'):\n",
    "                file_path = os.path.join(large_trade_folder, filename)\n",
    "                file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "                small_uploads.append({\n",
    "                    \"type\": \"file\",\n",
    "                    \"local\": file_path,\n",
    "                    \"remote\": f\"raw/trade_large/{filename}\",\n",
    "                    \"name\": f\"Large Trade File: {filename[:50]}...\",\n",
    "                    \"size\": file_size\n",
    "                })\n",
    "    \n",
    "    # Upload everything\n",
    "    print(f\"\\nüì§ Starting upload of {len(small_uploads)} items...\\n\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, task in enumerate(small_uploads, 1):\n",
    "        print(f\"[{i}/{len(small_uploads)}] üìÅ {task['name']}\")\n",
    "        \n",
    "        # Check if path exists\n",
    "        if not os.path.exists(task['local']):\n",
    "            print(f\"   ‚ö† Skipping - path not found: {task['local']}\")\n",
    "            continue\n",
    "        \n",
    "        # Show size info\n",
    "        if task['type'] == 'folder':\n",
    "            size_mb = get_folder_size(task['local'])\n",
    "            print(f\"   üìä Size: {size_mb:.2f} MB\")\n",
    "        elif 'size' in task:\n",
    "            print(f\"   üìä Size: {task['size']:.2f} MB\")\n",
    "        \n",
    "        # Upload\n",
    "        if task['type'] == 'file':\n",
    "            success = upload_with_retry(\n",
    "                api,\n",
    "                api.upload_file,\n",
    "                path_or_fileobj=task['local'],\n",
    "                path_in_repo=task['remote'],\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"dataset\"\n",
    "            )\n",
    "        else:  # folder\n",
    "            success = upload_with_retry(\n",
    "                api,\n",
    "                api.upload_folder,\n",
    "                folder_path=task['local'],\n",
    "                path_in_repo=task['remote'],\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"dataset\"\n",
    "            )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"   ‚úì Successfully uploaded\\n\")\n",
    "            successful += 1\n",
    "        else:\n",
    "            print(f\"   ‚úó Upload failed\\n\")\n",
    "            failed += 1\n",
    "        \n",
    "        # Small delay between uploads\n",
    "        if i < len(small_uploads):\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä UPLOAD SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úì Successful: {successful}\")\n",
    "    print(f\"‚úó Failed: {failed}\")\n",
    "    print(f\"üì¶ Repository: https://huggingface.co/datasets/{repo_id}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if failed > 0:\n",
    "        print(\"\\n‚ö† Some uploads failed. You can re-run this script to retry failed uploads.\")\n",
    "    else:\n",
    "        print(\"\\nüéâ All data uploaded successfully!\")\n",
    "        print(\"\\nüí° Next steps:\")\n",
    "        print(\"   1. Delete local data to free up space\")\n",
    "        print(\"   2. Use the data in your code with:\")\n",
    "        print(f\"\\n   from huggingface_hub import hf_hub_download\")\n",
    "        print(f\"   file = hf_hub_download(\")\n",
    "        print(f\"       repo_id='{repo_id}',\")\n",
    "        print(f\"       filename='raw/commodity_prices/extracted/cmoMonthly_monthly_prices.csv',\")\n",
    "        print(f\"       repo_type='dataset'\")\n",
    "        print(f\"   )\")\n",
    "        print(f\"   df = pd.read_csv(file)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
